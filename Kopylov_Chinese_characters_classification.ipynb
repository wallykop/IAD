{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopylov_Chinese.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wallykop/IAD_SMMO_2019/blob/master/Kopylov_Chinese_characters_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxq1xfaiiEmc",
        "colab_type": "text"
      },
      "source": [
        "## ИАД KAGGLE картинки\n",
        "Решение для [соревнования](https://www.kaggle.com/c/chinese-char-recognition-smmo19/overview) на курсе СММО ИАД 2019\n",
        "\n",
        "[Папка](https://drive.google.com/drive/folders/1RvYnd7FASNHpFTkkiU5Z1Qv3LmyyV1So?usp=sharing) с файлами для загрузки "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wxA8JcoD0ad",
        "colab_type": "code",
        "outputId": "f814ebc4-48d9-44b3-e66b-0b06295fe864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==2.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.1.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.1.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 56.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.1)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 61.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.17.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.11.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.33.6)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.16.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.21.0)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/f8/84b5771faec3eba9fe0c91c8c5896364a8ba08852c0dea5ad2025026dd95/google_auth-1.10.0-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (42.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0) (2.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.3.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.24.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.2.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.0.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.8)\n",
            "\u001b[31mERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.10.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, google-auth, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed google-auth-1.10.0 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3eN4po136K7",
        "colab_type": "code",
        "outputId": "02f1e3ee-4417-40d8-9d2f-1afb387a6e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "tf.version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'tensorflow_core._api.v2.version' from '/usr/local/lib/python3.6/dist-packages/tensorflow_core/_api/v2/version/__init__.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R92XZEh44BpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train = np.load(\"/content/drive/My Drive/Chinese_characters_Kopylov/train-1.npy\", allow_pickle=True)\n",
        "for i in range(2, 5):\n",
        "    t = np.load(f\"/content/drive/My Drive/Chinese_characters_Kopylov/train-{i}.npy\", allow_pickle=True)\n",
        "    data_train = np.concatenate([data_train, t])\n",
        "data_test = np.load(\"/content/drive/My Drive/Chinese_characters_Kopylov/test.npy\", allow_pickle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jXckA_rFZAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_to_id = {char: id for id, char in enumerate (np.unique([char for _, char in data_train]))}\n",
        "id_to_char = {id: char for id, char in enumerate (np.unique([char for _, char in data_train]))}\n",
        "\n",
        "val_size = 0.2\n",
        "batch_size = 128\n",
        "\n",
        "def train_gen():\n",
        "    for img, label in data_train[int(len(data_train) * val_size):]:\n",
        "        img = img[..., None] \n",
        "        yield img, char_to_id[label]\n",
        "\n",
        "def val_gen():\n",
        "    for img, label in data_train[:int(len(data_train) * val_size)]:\n",
        "        img = img[..., None] \n",
        "        yield img, char_to_id[label]\n",
        "\n",
        "def test_gen():\n",
        "    for img in data_test:\n",
        "        img = img[:, :, np.newaxis] \n",
        "        yield img\n",
        "\n",
        "def aug(img):\n",
        "  tf.image.random_contrast(img)\n",
        "  tf.image.random_brightness(img,max_delta=0.1)\n",
        "  tf.image.random_saturation(img)\n",
        "\n",
        "def preprocess_train(img, label):\n",
        "  img = tf.image.resize(img, (64,64))\n",
        "  img = tf.cast(img, tf.float32)\n",
        "  img = (img - 127.5) / 127.5\n",
        "  if np.random.rand() < 0.15:\n",
        "    img = aug(img)\n",
        "  label = tf.one_hot(label, 1000)\n",
        "  return img, label\n",
        "\n",
        "def preprocess_test(img):\n",
        "  img = tf.image.resize(img, (64,64))\n",
        "  img = tf.cast(img, tf.float32)\n",
        "  img = (img - 127.5) / 127.5\n",
        "  return img\n",
        "\n",
        "dataset_train = tf.data.Dataset.from_generator(train_gen,\n",
        "                                          output_types=(tf.float32, tf.int32),\n",
        "                                          output_shapes=((None,None,1), ())\n",
        "                                         ).map(preprocess_train, num_parallel_calls=-1).prefetch(-1).shuffle(1024).batch(batch_size).repeat()\n",
        "\n",
        "dataset_val = tf.data.Dataset.from_generator(val_gen,\n",
        "                                          output_types=(tf.float32, tf.int32),\n",
        "                                          output_shapes=((None,None,1), ())\n",
        "                                         ).map(preprocess_train, num_parallel_calls=-1).prefetch(-1).shuffle(1024).batch(batch_size).repeat()\n",
        "\n",
        "dataset_test = tf.data.Dataset.from_generator(test_gen,\n",
        "                                          output_types=tf.float32, \n",
        "                                          output_shapes=((None,None,1)),\n",
        "                                         ).map(preprocess_test, num_parallel_calls=-1).prefetch(-1).batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdDfP7-sISPT",
        "colab_type": "code",
        "outputId": "a0eaba3b-4252-48d3-c8ba-f9ce8e1a9133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, LeakyReLU, BatchNormalization\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
        "from keras.optimizers import adam\n",
        "import cv2\n",
        "from keras.models import save_model, load_model\n",
        "\n",
        "class ModelSaveCallback(keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, file_name):\n",
        "        super(ModelSaveCallback, self).__init__()\n",
        "        self.file_name = file_name\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        epoch += 1\n",
        "        model.save(self.file_name.format(epoch))\n",
        "        print(model.save(self.file_name.format(epoch)))\n",
        "\n",
        "def load_from_file(model_filename, last_epoch):\n",
        "  model = load_model(model_filename)\n",
        "  epoch = last_epoch\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGqQ5RL-sAoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Conv2D(filters=16, padding='same', kernel_size=(3,3), input_shape=(64,64,1), kernel_initializer='lecun_uniform', bias_initializer='lecun_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.1))\n",
        "\n",
        "    model.add(Conv2D(filters=16, padding='same', kernel_size=(3,3), input_shape=(64,64,1), kernel_initializer='lecun_uniform', bias_initializer='lecun_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.1))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "\n",
        "    model.add(Conv2D(filters=32, padding='same', kernel_size=(3,3), input_shape=(64,64,1), kernel_initializer='lecun_uniform', bias_initializer='lecun_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.1))\n",
        "\n",
        "    model.add(Conv2D(filters=32, padding='same', kernel_size=(3,3), input_shape=(64,64,1), kernel_initializer='lecun_uniform', bias_initializer='lecun_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.1))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "\n",
        "    model.add(Conv2D(filters=64, padding='same', kernel_size=(3,3), input_shape=(64,64,1), kernel_initializer='lecun_uniform', bias_initializer='lecun_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.1))\n",
        "\n",
        "    model.add(Conv2D(filters=64, padding='same', kernel_size=(3,3), input_shape=(64,64,1), kernel_initializer='lecun_uniform', bias_initializer='lecun_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.1))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "\n",
        "    model.add(Conv2D(filters=128, padding='same', kernel_size=(3,3), input_shape=(64,64,1), kernel_initializer='lecun_uniform', bias_initializer='lecun_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.1))\n",
        "\n",
        "    model.add(Conv2D(filters=128, padding='same', kernel_size=(3,3), input_shape=(64,64,1), kernel_initializer='lecun_uniform', bias_initializer='lecun_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.1))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(1024, kernel_initializer='lecun_uniform', bias_initializer='lecun_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.1))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Dense(1000, kernel_initializer='lecun_uniform', bias_initializer='lecun_uniform'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9h3YMJO--dS",
        "colab_type": "code",
        "outputId": "4ddd0ce2-0ab0-411c-df90-36b5090515d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = make_model()\n",
        "\n",
        "print(model.summary())\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = tf.keras.optimizers.Adam(lr = 0.001, decay = 0), metrics = [\"accuracy\"])\n",
        "\n",
        "model_filename = 'weights_{0:02d}.hdf5'\n",
        "\n",
        "def time_decay(epoch, initial_lrate):\n",
        "    decay_rate = 0.01\n",
        "    new_lrate = initial_lrate/(1+decay_rate*(epoch))\n",
        "    return new_lrate\n",
        "\n",
        "lrate = tf.keras.callbacks.LearningRateScheduler(time_decay,verbose=1)\n",
        "\n",
        "def schedule(ind):\n",
        "    a = [0.0001, 0.00005, 0.00005, 0.00001, 0.00001, 0.000005, 0.000001]\n",
        "    return a[ind]\n",
        "\n",
        "lr = tf.keras.callbacks.LearningRateScheduler(schedule)\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                           patience=7,\n",
        "                           verbose=1,\n",
        "                           min_delta=1e-4)\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = 0.5, patience = 2, min_lr = 0.0001)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 64, 64, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 64, 64, 16)        64        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 64, 64, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 64, 64, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 64, 64, 16)        64        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 64, 64, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 32, 32, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 32, 32, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 32, 32, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1000)              1025000   \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1000)              0         \n",
            "=================================================================\n",
            "Total params: 3,426,424\n",
            "Trainable params: 3,421,416\n",
            "Non-trainable params: 5,008\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLxFTbowIaxg",
        "colab_type": "code",
        "outputId": "c7cfd369-5f02-41d2-f2b3-a610e59b698c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "history = model.fit(dataset_train, \n",
        "                    steps_per_epoch=500, \n",
        "                    epochs = 1,\n",
        "                    validation_steps=100, \n",
        "                    validation_data=dataset_val, \n",
        "                    verbose = 1, \n",
        "                    callbacks = [ModelSaveCallback(model_filename), lr,\n",
        "                                 #lrate, \n",
        "                                 reduce_lr,\n",
        "                                 early_stop])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 500 steps, validate for 100 steps\n",
            "499/500 [============================>.] - ETA: 2s - loss: 6.6465 - accuracy: 0.0091None\n",
            "500/500 [==============================] - 1068s 2s/step - loss: 6.6449 - accuracy: 0.0092 - val_loss: 7.5027 - val_accuracy: 0.0011\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J509H1q04TLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = model.predict(dataset_test)\n",
        "submission = submission.argmax(1)\n",
        "result = pd.read_csv('/content/drive/My Drive/kaggle/random_labels.csv')\n",
        "result['Category'] = submission\n",
        "result['Category'] = result['Category'].apply(lambda x: id_to_char[x])\n",
        "result.to_csv('predictions.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2EtvSIZ5Kfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}